---
title: "ABM_analysis"
author: "SKK"
date: "2023-05-25"
output: html_document
---

# Investigate ABM

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(tidyverse,
               here,
               network,
               igraph,
               ggraph,
               tidygraph,
               patchwork)
```

## Table of content


```{r Load data}
stat <- tibble(expand.grid(iteration = seq(1,100),
                           table = seq(1,3),
                           network = seq(1,2)))
stat <- stat %>% 
  mutate(
                           pre_post = rep(vector("list", length = 600)),
                           degree = NA,
                           degree_mean = NA,
                           degree_dist = NA,
                           transitivity = NA,
                           between = NA,
                           eigen_cen = NA,
                           pre=rep(vector("list", length = 600)),
                           trans_mean = NA,
                           between_mean = NA,
                           eigen_cen_mean = NA
                           )
              
#rename 1:3 to long,round, square
for (row in 1:nrow(stat)){
stat$table[row] <- if (stat$table[row] == 1) "long" else if (stat$table[row] == 2) "round" else "square"
}

for (row in 1:nrow(stat)){
stat$network[row] <- if (stat$network[row] == 1) "pre" else "post" 
}

stat <- arrange(stat, iteration, table)

for (row in 1:nrow(df_collected)){
  #make network graphs
  temp <- df_collected[[3]][[row]]
  edges <- temp$ID
  nodes_pre <- df_collected[[4]][[row]] %>% filter(weight!=0)
  nodes_post <- df_collected[[5]][[row]] %>% filter(weight!=0)
  pre_igraph <- graph_from_data_frame(d = nodes_pre,
                        vertices = edges,
                        directed = TRUE)
  post_igraph <-
  graph_from_data_frame(d = nodes_post,
                        vertices = edges,
                        directed = TRUE)
  #save
  pre_index <- which(stat$iteration == df_collected$iteration[row] & stat$table == df_collected$table_type[row])[1]
  post_index <- which(stat$iteration == df_collected$iteration[row] & stat$table == df_collected$table_type[row])[2]
  #network graphs
  stat$pre_post[[pre_index]] <- pre_igraph#as_long_data_frame(pre_igraph, mode="all"))
  stat$pre_post[[post_index]] <- post_igraph#as_long_data_frame(degree(post_igraph, mode="all"))
  stat$pre[[post_index]] <- pre_igraph
  #make degree
  stat$degree[[pre_index]] <- as.data.frame(degree(pre_igraph, mode="all"))
  stat$degree[[post_index]] <- as.data.frame(degree(post_igraph, mode="all"))
  #mean degree
  stat$degree_mean[[pre_index]] <- as.numeric(mean(degree(pre_igraph)))
  stat$degree_mean[[post_index]] <- as.numeric(mean(degree(post_igraph)))
  #degree distribution
  stat$degree_dist[[pre_index]] <- as.data.frame(degree_distribution(pre_igraph))
  stat$degree_dist[[post_index]] <- as.data.frame(degree_distribution(post_igraph))
  #transitivity
  stat$transitivity[[pre_index]] <- as.data.frame(transitivity(pre_igraph, type = 'local'))
  stat$transitivity[[post_index]] <- as.data.frame(transitivity(post_igraph, type = 'local'))
  #trans_mean
  stat$trans_mean[[pre_index]] <- as.numeric(mean(stat$transitivity[[pre_index]][[1]]))
  stat$trans_mean[[post_index]] <- as.numeric(mean(stat$transitivity[[post_index]][[1]]))

  #betweenness
  stat$between[[pre_index]] <- as.data.frame(betweenness(pre_igraph))
  stat$between[[post_index]] <- as.data.frame(betweenness(post_igraph))
    #between_mean
  stat$between_mean[[pre_index]] <- as.numeric(mean(stat$between[[pre_index]][[1]]))
  stat$between_mean[[post_index]] <- as.numeric(mean(stat$between[[post_index]][[1]]))

  #eigen centrality
  stat$eigen_cen[[pre_index]] <- as.data.frame(eigen_centrality(pre_igraph)$vector)
  stat$eigen_cen[[post_index]] <- as.data.frame(eigen_centrality(post_igraph)$vector)
  #eigen_cen_mean
  stat$eigen_cen_mean[[pre_index]] <- as.numeric(mean(stat$eigen_cen[[pre_index]][[1]]))
  stat$eigen_cen_mean[[post_index]] <- as.numeric(mean(stat$eigen_cen[[post_index]][[1]]))

}

post_index <- which(stat$network=="post")
stat_post <- stat[post_index,]
stat_post <- stat_post %>% 
  mutate(table=as.factor(table))

```

```{r frequentist/bayes}

library(tidyverse)
pacman::p_load(rstanarm,rstan)

# Step 3: Split the data into training and testing sets
set.seed(123)
train_indices <- sample(nrow(stat_post), nrow(stat_post) * 0.7) # 70% for training
train_data <- stat_post[train_indices, ]
test_data <- stat_post[-train_indices, ]

# Step 4: Build the regression model
model1 <- stan_glmer(eigen_cen_mean ~ degree_mean + trans_mean + between_mean  + (1|table), data = train_data)
summary(model1)
#weird stuff
```
#t.test
```{r}
ttest <- tibble(expand.grid(iteration = seq(1,100),
                           table = seq(1,3),
                           degree = NA,
                           transitivity = NA,
                           between = NA,
                           eigen_cen = NA,
                           degree_dis = NA
                           )
                )

#rename 1:3 to long,round, square
for (row in 1:nrow(ttest)){
ttest$table[row] <- if (ttest$table[row] == 1) "long" else if (ttest$table[row] == 2) "round" else "square"
}

ttest <- ttest %>% 
  mutate(table=as.factor(table))

for (row in 1:nrow(ttest)){
  pre_index <- which(stat$iteration == ttest$iteration[row] & stat$table == ttest$table[row] &stat$network=="pre")
  post_index <- which(stat$iteration == ttest$iteration[row] & stat$table == ttest$table[row] & stat$network == "post")
  
  #degree
  degpre <- stat$degree[pre_index][[1]]$`degree(pre_igraph, mode = "all")`
  degpos <- stat$degree[post_index][[1]]$`degree(post_igraph, mode = "all")`
  t <- t.test(degpos,degpre)
   ttest$degree[[row]] <- t$p.value
   
  #degree_distribution
  degpre <- stat$degree_dist[pre_index][[1]]$`degree_distribution(pre_igraph)`
  degpos <- stat$degree_dist[post_index][[1]]$`degree_distribution(post_igraph)`
  t <- t.test(degpos,degpre)
   ttest$degree_dis[[row]] <- t$p.value
   
  #transitivity
  degpre <- stat$transitivity[pre_index][[1]]$`transitivity(pre_igraph, type = "local")`
  degpos <- stat$transitivity[post_index][[1]]$`transitivity(post_igraph, type = "local")`
  t <- t.test(degpos,degpre)
   ttest$transitivity[[row]] <- t$p.value

  #betweenness
  degpre <- stat$between[pre_index][[1]]$`betweenness(pre_igraph)`
  degpos <- stat$between[post_index][[1]]$`betweenness(post_igraph)`
  t <- t.test(degpos,degpre)
   ttest$between[[row]] <- t$p.value
   
  #eigen_centrality
  degpre <- stat$eigen_cen[pre_index][[1]]$`eigen_centrality(pre_igraph)$vector`
  degpos <- stat$eigen_cen[post_index][[1]]$`eigen_centrality(post_igraph)$vector`
  t <- t.test(degpos,degpre)
   ttest$eigen_cen[[row]] <- t$p.value

}

ttest <- ttest %>% 
  mutate(table=as.factor(table))

sum_ttest <- ttest %>% 
  mutate(degree=as.numeric(round(degree,digits=3))) %>% 
  mutate(transitivity=as.numeric(round(transitivity,digits=3))) %>% 
  mutate(between=as.numeric(round(between,digits=3))) %>% 
  mutate(eigen_cen=as.numeric(round(eigen_cen,digits=3))) %>% 
  mutate(degree_dis=as.numeric(round(degree_dis,digits = 3)))


# Function to count occurrences below 0.05
count_below_threshold <- function(column) {
  sum(column < 0.05)
}

# Apply the function to each column and create the table
rl <- sum_ttest[1:100,]
rl <- apply(rl[, cols.num], 2, count_below_threshold)

rs <- sum_ttest[201:300,]
rs <- apply(rs[, cols.num], 2, count_below_threshold)

rr <- sum_ttest[101:200,]
rr <- apply(rr[, cols.num], 2, count_below_threshold)

degree <- c(rl[1][[1]],rs[1][[1]],rr[1][[1]])
degree_dis <- c(rl[5][[1]],rs[5][[1]],rr[5][[1]])
transitivity <- c(rl[2][[1]],rs[2][[1]],rr[2][[1]])
between <- c(rl[3][[1]],rs[3][[1]],rr[3][[1]])
eigen_cen <- c(rl[4][[1]],rs[4][[1]],rr[4][[1]])

table <- c("long","square","round")

results <- tibble(table = table,degree=degree,degree_distribution=degree_dis,transitivity=transitivity,eigen_centrality=eigen_cen,betweenness=between)

tapply(sum_ttest$table, count_below_threshold(cols.num))
result <- tapply(sum_ttest$degree < 0.05, sum_ttest$Type, count_below_threshold)


strand_df[[5]][[1]][strand_df[[5]][[1]] > 0] <- 1



# Clustering coefficient

transitivity(network_cleaned)
transitivity(CogSci_2019_n1_igraph)

Transitivity_ABM <- transitivity(network_cleaned, type = 'local')
Transitivity_2019 <-transitivity(CogSci_2019_n1_igraph, type = 'local')

t.test(Transitivity_ABM, Transitivity_2019) 
## Centrality

mean(betweenness(network_cleaned))
rethinking::dens(betweenness(network_cleaned))+
  title("ABM Betweenness Centrality")
mean(betweenness(CogSci_2019_n1_igraph))
rethinking::dens(betweenness(CogSci_2019_n1_igraph))+
  title("2019 Data Betweenness Centrality")

Betweenness_ABM <- betweenness(network_cleaned)
Betweenness_2019 <- betweenness(CogSci_2019_n1_igraph)

t.test(Betweenness_ABM, Betweenness_2019) 


mean(eigen_centrality(network_cleaned)$vector)
mean(eigen_centrality(CogSci_2019_n1_igraph)$vector)

Eigen_ABM <- eigen_centrality(network_cleaned)$vector
Eigen_2019 <- eigen_centrality(CogSci_2019_n1_igraph)$vector

t.test(Eigen_ABM, Eigen_2019)


```


## 60 Days : Joint and Big

```{r}
#Comparing only small and big events for 60 days

network_cleaned_joint60 <- network_cleaned_joint_60
network_cleaned_big60 <- network_cleaned_big_60

par(mfrow=c(1,2))

mean(degree(network_cleaned_joint60))
mean(degree(network_cleaned_big60))

J60_D <- degree(network_cleaned_joint60)
BE60_D <- degree(network_cleaned_big60) 

t.test(BE60_D, J60_D)

mean(degree_distribution(network_cleaned_joint60))
mean(degree_distribution(network_cleaned_big60))

DegreeDistr_J60 <- degree_distribution(network_cleaned_joint60)
DegreeDistr_BE60 <- degree_distribution(network_cleaned_big60)

t.test(DegreeDistr_ABM, DegreeDistr_2019)

# Degree

rethinking::dens(degree_distribution(network_cleaned_joint60))+
  title("Small Event Degree Distribution 60")
rethinking::dens(degree_distribution(network_cleaned_big60))+
  title("Big Event Degree Distribution 60")



# Average path length

mean_distance(network_cleaned_joint60, directed = TRUE, unconnected = TRUE)
mean_distance(network_cleaned_big60, directed = TRUE, unconnected = TRUE)

# Clustering coefficient

transitivity(network_cleaned_joint60)
transitivity(network_cleaned_big60)

Transitivity_J60 <- transitivity(network_cleaned_joint60, type = 'local')
Transitivity_BE60 <-transitivity(network_cleaned_big60, type = 'local')

t.test(Transitivity_BE60, Transitivity_J60) 
## Centrality

mean(betweenness(network_cleaned_joint60))
rethinking::dens(betweenness(network_cleaned_joint60))+
  title("Small Event 60 Betweenness Centrality")
mean(betweenness(network_cleaned_big60))
rethinking::dens(betweenness(network_cleaned_big60))+
  title("Big Event 60 Betweenness Centrality")

Betweenness_J60 <- betweenness(network_cleaned_joint60)
Betweenness_BE60 <- betweenness(network_cleaned_big60)

t.test(Betweenness_BE60, Betweenness_J60) 


mean(eigen_centrality(network_cleaned_joint60)$vector)
mean(eigen_centrality(network_cleaned_big60)$vector)

Eigen_J60 <- eigen_centrality(network_cleaned_joint60)$vector
Eigen_BE60 <- eigen_centrality(network_cleaned_big60)$vector

t.test(Eigen_BE60, Eigen_J60)


```

#60 days : Small and Big
```{r}
#Comparing only small and big events for 60 days

network_cleaned_small60 <- network_cleaned_small_60
network_cleaned_big60 <- network_cleaned_big_60

par(mfrow=c(1,2))

mean(degree(network_cleaned_small60))
mean(degree(network_cleaned_big60))

SE60_D <- degree(network_cleaned_small60)
BE60_D <- degree(network_cleaned_big60) 

t.test(BE60_D, SE60_D)

mean(degree_distribution(network_cleaned_small60))
mean(degree_distribution(network_cleaned_big60))

DegreeDistr_SE60 <- degree_distribution(network_cleaned_small60)
DegreeDistr_BE60 <- degree_distribution(network_cleaned_big60)

#t.test(DegreeDistr_ABM, DegreeDistr_2019)

# Degree

rethinking::dens(degree_distribution(network_cleaned_small60))+
  title("Small Event Degree Distribution 60")
rethinking::dens(degree_distribution(network_cleaned_big60))+
  title("Big Event Degree Distribution 60")



# Average path length

mean_distance(network_cleaned_small60, directed = TRUE, unconnected = TRUE)
mean_distance(network_cleaned_big60, directed = TRUE, unconnected = TRUE)

# Clustering coefficient

transitivity(network_cleaned_small60)
transitivity(network_cleaned_big60)

Transitivity_SE60 <- transitivity(network_cleaned_small60, type = 'local')
Transitivity_BE60 <-transitivity(network_cleaned_big60, type = 'local')

t.test(Transitivity_BE60, Transitivity_SE60) 
## Centrality

mean(betweenness(network_cleaned_small60))
rethinking::dens(betweenness(network_cleaned_small60))+
  title("Small Event 60 Betweenness Centrality")
mean(betweenness(network_cleaned_big60))
rethinking::dens(betweenness(network_cleaned_big60))+
  title("Big Event 60 Betweenness Centrality")

Betweenness_SE60 <- betweenness(network_cleaned_small60)
Betweenness_BE60 <- betweenness(network_cleaned_big60)

t.test(Betweenness_BE60, Betweenness_SE60) 


mean(eigen_centrality(network_cleaned_small60)$vector)
mean(eigen_centrality(network_cleaned_big60)$vector)

Eigen_SE60 <- eigen_centrality(network_cleaned_small60)$vector
Eigen_BE60 <- eigen_centrality(network_cleaned_big60)$vector

t.test(Eigen_BE60, Eigen_SE60)


```



## 120 Days : Joint and Big

```{r}
ttest <- stat %>% 
  
#Comparing only small and big events for 60 days

network_cleaned_joint120 <- network_cleaned_joint_120
network_cleaned_big120 <- network_cleaned_big_120

par(mfrow=c(1,2))

mean(degree(network_cleaned_joint120))
mean(degree(network_cleaned_big120))

J120_D <- degree(network_cleaned_joint120)
BE120_D <- degree(network_cleaned_big120) 

t.test(BE120_D, J120_D)

mean(degree_distribution(network_cleaned_joint120))
mean(degree_distribution(network_cleaned_big120))

DegreeDistr_J60 <- degree_distribution(network_cleaned_joint120)
DegreeDistr_BE60 <- degree_distribution(network_cleaned_big120)

t.test(DegreeDistr_BE60, DegreeDistr_J60)

# Degree

rethinking::dens(degree_distribution(network_cleaned_joint120))+
  title("Small Event Degree Distribution 120")
rethinking::dens(degree_distribution(network_cleaned_big120))+
  title("Big Event Degree Distribution 120")



# Average path length

mean_distance(network_cleaned_joint120, directed = TRUE, unconnected = TRUE)
mean_distance(network_cleaned_big120, directed = TRUE, unconnected = TRUE)

# Clustering coefficient

transitivity(network_cleaned_joint120)
transitivity(network_cleaned_big120)

Transitivity_J120 <- transitivity(network_cleaned_joint120, type = 'local')
Transitivity_BE120 <-transitivity(network_cleaned_big120, type = 'local')

t.test(Transitivity_BE120, Transitivity_J120) 
## Centrality

mean(betweenness(network_cleaned_joint120))
rethinking::dens(betweenness(network_cleaned_joint120))+
  title("Small Event 120 Betweenness Centrality")
mean(betweenness(network_cleaned_big120))
rethinking::dens(betweenness(network_cleaned_big120))+
  title("Big Event 120 Betweenness Centrality")

Betweenness_J120 <- betweenness(network_cleaned_joint120)
Betweenness_BE120 <- betweenness(network_cleaned_big120)

t.test(Betweenness_BE120, Betweenness_J120) 


mean(eigen_centrality(network_cleaned_joint120)$vector)
mean(eigen_centrality(network_cleaned_big120)$vector)

Eigen_J120 <- eigen_centrality(network_cleaned_joint120)$vector
Eigen_BE120 <- eigen_centrality(network_cleaned_big120)$vector

t.test(Eigen_BE120, Eigen_J120)


```
#120 days


```{r}
#Comparing only small and big events for 120 days

network_cleaned_small120 <- network_cleaned_small_120
network_cleaned_big120 <- network_cleaned_big_120

par(mfrow=c(1,2))

mean(degree(network_cleaned_small120))
mean(degree(network_cleaned_big120))

SE120_D <- degree(network_cleaned_small120)
BE120_D <- degree(network_cleaned_big120) 

t.test(BE120_D, SE120_D)

mean(degree_distribution(network_cleaned_small120))
mean(degree_distribution(network_cleaned_big120))

DegreeDistr_SE120 <- degree_distribution(network_cleaned_small120)
DegreeDistr_BE120 <- degree_distribution(network_cleaned_big120)

t.test(DegreeDistr_BE120, DegreeDistr_SE120)

# Degree

rethinking::dens(degree_distribution(network_cleaned_small120))+
  title("Small Event Degree Distribution 120")
rethinking::dens(degree_distribution(network_cleaned_big120))+
  title("Big Event Degree Distribution 120")



# Average path length

mean_distance(network_cleaned_small120, directed = TRUE, unconnected = TRUE)
mean_distance(network_cleaned_big120, directed = TRUE, unconnected = TRUE)

# Clustering coefficient

transitivity(network_cleaned_small120)
transitivity(network_cleaned_big120)

Transitivity_SE120 <- transitivity(network_cleaned_small120, type = 'local')
Transitivity_BE120 <-transitivity(network_cleaned_big120, type = 'local')

t.test(Transitivity_BE120, Transitivity_SE120) 
## Centrality

mean(betweenness(network_cleaned_small120))
rethinking::dens(betweenness(network_cleaned_small120))+
  title("Small Event 120 Betweenness Centrality")
mean(betweenness(network_cleaned_big120))
rethinking::dens(betweenness(network_cleaned_big120))+
  title("Big Event 120 Betweenness Centrality")

Betweenness_SE120 <- betweenness(network_cleaned_small120)
Betweenness_BE120 <- betweenness(network_cleaned_big120)

t.test(Betweenness_BE120, Betweenness_SE120) 


mean(eigen_centrality(network_cleaned_small120)$vector)
mean(eigen_centrality(network_cleaned_big120)$vector)

Eigen_SE120 <- eigen_centrality(network_cleaned_small120)$vector
Eigen_BE120 <- eigen_centrality(network_cleaned_big120)$vector

t.test(Eigen_BE120, Eigen_SE120)
```


# 4 Community Detection

```{r}
#making the networks look the same for easier comparing
E(network_cleaned)$arrow.mode <- 0
V(network_cleaned)$label <- ""

# Community detection
wc1 <- cluster_walktrap(network_cleaned)
# wc1 <- cluster_infomap(as.undirected(network_cleaned))
modularity(wc1)
membership(wc1)

wc2 <- cluster_walktrap(CogSci_2019_n1_igraph)
modularity(wc2)
membership(wc2)


par(mfrow=c(1,2))

plot(wc1, network_cleaned, layout = layout_nicely)
plot(wc2, CogSci_2019_n1_igraph)


# Community detection (by optimizing modularity over partitions):
clp <- cluster_louvain(as.undirected(network_cleaned))
plot(clp, network_cleaned, layout = layout_nicely, edge.arrow.size = 0.2)


clp <- cluster_louvain(as.undirected(CogSci_2019_n1_igraph))
plot(clp, CogSci_2019_n1_igraph, layout = layout_nicely, edge.arrow.size = 0.2)


#clop <- cluster_optimal(CogSci_2019_n1_igraph)
#plot(clop, CogSci_2019_n1_igraph)

#par(mfrow=c(1,1))
```


#STRAND
```{r STRAND}
# library(devtools)
# install_github("ctross/STRAND")
library(STRAND)
```

- Use base priors for the first party out of 300
- run for each party
  - an exposure matrix with the ID's and weights of the maximum outcome (IDxID with 50)
  - 
  
  
outcome = list(inGoing) #matrix of post network
exposte_list = list(exposre) #matrix of max friends weight
dyad = list(friend = pre_network,
#satisfaction_distance = satisfaction_dist,
#outcome_distance = outcome_dist
)

blocks = df %>%  select(sex, outcome, satisfaction) %>% mutate(sex=as.factor(sex)) %>% as.data.frame()
individual = df %>% select(!c("name", "sex"))


dat = make_strand_data(self_report = outcome,
block_covariets = blocks,
individual_covariaets = individual,
dyadic_covariates = dyad,
exposure = exposure_list,
outcome_mode = "binomial")

f = fit_latent_network_model(
data=dat,
fpr_regression = ~ 1,
rtt_regression = ~ 1,
theta_regression = ~ 1,
focal_regression = ~ 1,
target_regression = ~ 1,
dyad_regression = ~ 1,
mode="mcmc")

r = summarize_strand_results(f)





```{r Matrices}
#try one party
strand_df <- df_collected[1,]

strand_df <- strand_df %>% 
  mutate(table_type=as.factor(table_type))

temp <- strand_df[[3]][[1]]
IDs <- temp$ID

# Create an empty matrix with IDxID dimensions
exposure <- matrix(0, nrow = length(IDs), ncol = length(IDs), dimnames = list(IDs, IDs))

# Fill the matrix with the value 50
exposure[] <- 50

blocks = strand_df %>% select(table_type) %>% as.data.frame()
individual = temp %>% select(c("ie", "satisfaction_baseline", "weight_time", "weight_interest", "moves", "n_closefriends"))

for (row in 1:nrow(strand_df)){
  #pre
  temp <- strand_df[[4]][row][[1]]
  strand_df[[4]][row][[1]] <- temp %>%
  pivot_wider(names_from = to, values_from = weight, values_fill = 0) %>%
  column_to_rownames(var = "from") %>%
  as.matrix()
  #post
  temp <- strand_df[[5]][row][[1]]
  strand_df[[5]][row][[1]] <- temp %>%
  pivot_wider(names_from = to, values_from = weight, values_fill = 0) %>%
  column_to_rownames(var = "from") %>%
  as.matrix()
  
}

#binary outcome for pre and postnetwork
strand_df[[5]][[1]][strand_df[[5]][[1]] > 0] <- 1
strand_df[[4]][[1]][strand_df[[4]][[1]] > 0] <- 1


#interest matrix

# Convert dataframe into a matrix
i = strand_df[[3]][[1]] %>% select(c("interest1","interest2","interest3"))
interest_matrix <- as.matrix(i)  # Exclude the ID column

# Create an empty matrix with IDxID dimensions
overlap_matrix <- matrix(0, nrow = length(IDs), ncol = length(IDs), dimnames = list(IDs, IDs))

# Compute the number of overlapping interests
for (i in 1:length(IDs)) {
  for (j in 1:length(IDs)) {
    if (i != j) {
      overlap_count <- sum(interest_matrix[i, ] %in% interest_matrix[j, ])
      overlap_matrix[i, j] <- overlap_count
    }
  }
}
```

```{r}
#make strand dataframe
outcome = list(strand_df[[5]][[1]]) #postnetwork

dyad = list(interests = overlap_matrix, 
            Friends = strand_df[[4]][[1]] #prenetwork
            )


indiv = tibble(IE = individual$ie, 
                    wf = individual$weight_time,  
                    wi = individual$weight_interest,
                    satisfaction = individual$satisfaction_baseline
                     )

exposure_list = list(exposure)

dat = make_strand_data(self_report = outcome, #why binary?
                       individual_covariates = indiv, 
                       dyadic_covariates = dyad,
                       exposure = exposure_list,
                       outcome_mode = "poisson")


fit = fit_latent_network_model(data=dat,
                                block_regression = ~ 1,
                                focal_regression = ~ IE + wf + wf + satisfaction,
                                target_regression = ~ IE + wf + wf + satisfaction,
                                dyad_regression = ~ interests + Friends,
                                fpr_regression = ~ 1,
                                rtt_regression = ~ 1,
                                theta_regression = ~ 1,
                                mode="mcmc",
                                return_predicted_network = TRUE,
                                stan_mcmc_parameters = list(seed = 1, chains = 1, parallel_chains = 1, refresh = 1, iter_warmup = 100,
                                iter_sampling = 100, max_treedepth = NULL, adapt_delta = NULL)
                                              )
```



## Mess

```{r}
par(mfrow=c(1,2))

mean(degree(pre_igraph))
mean(degree(post_igraph))
sd(degree(pre_igraph))

preD <- degree(pre_igraph)
postD <- degree(post_igraph) 

t.test(preD, postD)

mean(degree_distribution(pre_igraph))
mean(degree_distribution(post_igraph))

DegreeDistr_ABM <- degree_distribution(pre_igraph)
DegreeDistr_2019 <- degree_distribution(post_igraph)

t.test(DegreeDistr_ABM, DegreeDistr_2019)

# Degree

rethinking::dens(degree_distribution(pre_igraph))+
  title("ABM Degree Distribution")
rethinking::dens(degree_distribution(post_igraph))+
  title("2019 Data Degree Distribution")



# Average path length

mean_distance(pre_igraph, directed = TRUE, unconnected = TRUE)
mean_distance(post_igraph, directed = TRUE, unconnected = TRUE)

# Clustering coefficient

transitivity(pre_igraph)
transitivity(post_igraph)

Transitivity_ABM <- transitivity(pre_igraph, type = 'local')
Transitivity_2019 <-transitivity(post_igraph, type = 'local')

t.test(Transitivity_ABM, Transitivity_2019) 
## Centrality

mean(betweenness(pre_igraph))
rethinking::dens(betweenness(pre_igraph))+
  title("ABM Betweenness Centrality")
mean(betweenness(post_igraph))
rethinking::dens(betweenness(post_igraph))+
  title("2019 Data Betweenness Centrality")

Betweenness_ABM <- betweenness(pre_igraph)
Betweenness_2019 <- betweenness(post_igraph)

t.test(Betweenness_ABM, Betweenness_2019) 


mean(eigen_centrality(pre_igraph)$vector)
mean(eigen_centrality(post_igraph)$vector)

Eigen_ABM <- eigen_centrality(pre_igraph)$vector
Betweenness_2019 <- betweenness(post_igraph)

t.test(Eigen_ABM, Eigen_2019)


```



# 4 Community Detection

```{r}
#making the networks look the same for easier comparing
E(network_cleaned)$arrow.mode <- 0
V(network_cleaned)$label <- ""

# Community detection
wc1 <- cluster_walktrap(pre_igraph)
# wc1 <- cluster_infomap(as.undirected(network_cleaned))
modularity(wc1)
membership(wc1)

wc2 <- cluster_walktrap(post_igraph)
modularity(wc2)
membership(wc2)


par(mfrow=c(1,2))

plot(wc1, pre_igraph, layout = layout_nicely)
plot(wc2, post_igraph)


# Community detection (by optimizing modularity over partitions):
clp <- cluster_louvain(as.undirected(network_cleaned))
plot(clp, network_cleaned, layout = layout_nicely, edge.arrow.size = 0.2)


clp <- cluster_louvain(as.undirected(post_igraph))
plot(clp, post_igraph, layout = layout_nicely, edge.arrow.size = 0.2)


#clop <- cluster_optimal(post_igraph)
#plot(clop, post_igraph)

#par(mfrow=c(1,1))
```





```{r test hosts}
#check it out
# head(hosts)
#seems good

#could be fun to make a radar chart of the hosts
#https://www.datanovia.com/en/blog/beautiful-radar-chart-in-r-using-fmsb-and-ggplot-packages/
#opar <- par() 
# Define settings for plotting in a 3x4 grid, with appropriate margins:
#par(mar = rep(0.8,3))
#par(mfrow = c(3,3))
# Produce a radar-chart for each student
#for (i in 1:nrow(hosts)) {
 # radarchart(
  #  hosts[c(2:7, i), ],#hosts[ i,c(2:7)]
   # pfcol = c("#99999980",NA),
    #pcol= c(NA,2), plty = 1, plwd = 2,
#    title = row.names(hosts)[i]
 # )
#}
# Restore the standard par() settings
#par <- par(opar) 
```

```{r test interest overlap}
# par(mfrow=c(1,3))
# plot(density(hosts$interest1))
# plot(density(hosts$interest2))
# plot(density(hosts$interest3))
# #library(plyr)
# dplyr::count_(hosts, vars = c('interest1','interest2','interest3'))
# aggregate(ID ~ ., hosts, FUN = length)
# 
# dupes <- apply(hosts[,2:4], 1, function(x) any(duplicated(x)))
# dupes <- hosts[dupes, ]
# dup <- dupes[1,]
# stringr::str_count(dupes) #18 true #four characters, =28 procent had an overlap in at least two coloumns
```

```{r test interest overlap}
# #test if people have overlap in interests within themselves
# dupes <- apply(guests[,5:7], 1, function(x) any(duplicated(x)))
# stringr::str_count(dupes) #16 true #four characters, =25 procent had an overlap in at least two coloumns

```

```{r plot packages}
#Plotting Friendship network
pacman::p_load(tidyverse,
               here,
               network,
               igraph,
               ggraph,
               tidygraph,
               patchwork,
               bootnet)

```

```{r plot prenetwork}
pacman::p_load(tidyverse,
               here,
               network,
               igraph,
               ggraph,
               tidygraph,
               patchwork)
nodes <- party$ID

pre_igraph <-
  graph_from_data_frame(d = df_collected$pre_network[[1]],
                        vertices = nodes,
                        directed = TRUE)


pre_igraph <- simplify(pre_igraph, 
                                  remove.multiple = TRUE, 
                                  remove.loops = TRUE,
                                  edge.attr.comb = igraph_opt("edge.attr.comb"))



E(pre_igraph)$arrow.mode <- 0



V(pre_igraph)$label <- "" 

V(pre_igraph)$frame.color <- "white"
V(pre_igraph)$color <- "orange"
E(pre_igraph)$width <- edge_attr(pre_igraph)$weight/16 #this one adjusts the size of arrows 
#E(pre_igraph)$arrow.size <- 0.01
plot(pre_igraph,
     layout=layout_nicely,
     #vertex.label= V(pr,
     edge.arrow.size=0.01#,
     #vertex.label.color = "black",
     #vertex.size=node.size
     )

#ask Emma how to make their cool network plot
```

```{r plot network at round tables}
round <- expand.grid(from=round_tables$new_seat,to=round_tables$new_seat)
round$weight <- 0
round <- filter(round, from != to)
#Add weight to all the friendships we just created
for (i in 1:nrow(round_tables)){
  current_id <- round_tables$new_seat[i]
  temp <- round %>% filter(from==current_id)
   for (k in strsplit(round_tables$indices[i], ", ")[[1]]){
     temp2 <- temp %>% filter(to==k)
      temp2$weight <- temp2$weight + 1
       temp <- temp %>% filter(to!=k)
      temp <- rbind(temp2, temp)
  }
    round <- round %>% filter(from!=current_id)
    round <- rbind(round, temp)
}

# for visualisation
round <- round[!(round$weight==0),]
round_graph <-
  graph_from_data_frame(d = round,
                        vertices = round_tables$new_seat,
                        directed = TRUE)

plot(round_graph,
     layout = layout_nicely, # Explore layouts!
     edge.arrow.size = 0.2) 
```

```{r plot network at square tables}
square <- expand.grid(from=square_tables$new_seat,to=square_tables$new_seat)
square$weight <- 0
square <- filter(square, from != to)
#Add weight to all the friendships we just created
for (i in 1:nrow(square_tables)){
  current_id <- square_tables$new_seat[i]
  temp <- square %>% filter(from==current_id)
   for (k in strsplit(square_tables$indices[i], ", ")[[1]]){
     temp2 <- temp %>% filter(to==k)
      temp2$weight <- temp2$weight + 1
       temp <- temp %>% filter(to!=k)
      temp <- rbind(temp2, temp)
  }
    square <- square %>% filter(from!=current_id)
    square <- rbind(square, temp)
}

# for visualisation
square <- square[!(square$weight==0),]
square_graph <-
  graph_from_data_frame(d = square,
                        vertices = square_tables$new_seat,
                        directed = TRUE)

plot(square_graph,
     layout = layout_nicely, # Explore layouts!
     edge.arrow.size = 0.2) 
```

```{r plot network at long table}
long <- expand.grid(from=long_table$seat,to=long_table$seat)
long$weight <- 0
long <- filter(long, from != to)
#Add weight to all the friendships we just created
for (i in 1:nrow(long_table)){
  current_id <- long_table$seat[i]
  temp <- long %>% filter(from==current_id)
   for (k in strsplit(long_table$indices[i], ", ")[[1]]){
     temp2 <- temp %>% filter(to==k)
      temp2$weight <- temp2$weight + 1
       temp <- temp %>% filter(to!=k)
      temp <- rbind(temp2, temp)
  }
    long <- long %>% filter(from!=current_id)
    long <- rbind(long, temp)
}

# for visualisation
long <- long[!(long$weight==0),]
long_graph <-
  graph_from_data_frame(d = long,
                        vertices = long_table$seat,
                        directed = TRUE)

plot(long_graph,
     layout = layout_nicely, # Explore layouts!
     edge.arrow.size = 0.2) 

```

```{r plot post_network}
pacman::p_load(tidyverse,
               here,
               network,
               igraph,
               ggraph,
               tidygraph,
               patchwork)
nodes <- party$ID

post_igraph <-
  graph_from_data_frame(d =df_collected$post_network[[1]],
                        vertices = nodes,
                        directed = TRUE)


post_igraph <- simplify(post_igraph, 
                                  remove.multiple = TRUE, 
                                  remove.loops = TRUE,
                                  edge.attr.comb = igraph_opt("edge.attr.comb"))



E(post_igraph)$arrow.mode <- 0



V(post_igraph)$label <- "" 

V(post_igraph)$frame.color <- "white"
V(post_igraph)$color <- "orange"
E(post_igraph)$width <- edge_attr(post_igraph)$weight/16 #this one adjusts the size of arrows 
#E(post_igraph)$arrow.size <- 0.01
plot(post_igraph,
     layout=layout_nicely,
     #vertex.label= V(pr,
     edge.arrow.size=0.01#,
     #vertex.label.color = "black",
     #vertex.size=node.size
     )

#ask Emma how to make their cool network plot
``` 